{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODEK4IaF/wPvpSlckp7ToZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagoda222/LoLa---group-8/blob/main/dataset_triplets_1000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling 10,000 Triplets from SNLI Dataset\n",
        "\n",
        "This notebook demonstrates how to preprocess the SNLI dataset and sample **10,000 triplets** (30,000 rows). Each triplet consists of:\n",
        "- One **premise** shared by three rows.\n",
        "- Three **hypotheses** corresponding to labels `0` (entailment), `1` (neutral), and `2` (contradiction).\n",
        "\n",
        "**Steps:**\n",
        "1. Load the SNLI dataset\n",
        "2. Assign unique **triplet numbers** to valid triplets (groups of three rows with the same premise).\n",
        "3. Divide dataset into **10 equal-sized blocks** and randomly sample **1000 triplets** per block.\n",
        "4. Combine sampled triplets and save the final dataset as `sampled_snli_triplets_10000.csv`.\n",
        "\n",
        "This ensures uniform distribution and a clean triplet structure for further analysis."
      ],
      "metadata": {
        "id": "pOjp2kEvVgCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaT-osrcEvoX",
        "outputId": "a252af4b-14cb-4c82-9b08-df80837d85a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "RJv_Pa_8EW_7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "vjRstrM3bICG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"snli\")\n",
        "train_data = dataset['train'].to_pandas()\n",
        "\n",
        "# Rename columns for clarity\n",
        "train_data = train_data.rename(columns={\n",
        "    'sentence1': 'premise',\n",
        "    'sentence2': 'hypothesis',\n",
        "    'gold_label': 'label'\n",
        "})\n",
        "\n",
        "train_data = train_data[train_data['label'].notnull()]\n",
        "print(train_data.head())\n",
        "print(train_data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmKkLGKcGT15",
        "outputId": "438e1cce-0f87-4926-e603-bec8479c5722"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             premise  \\\n",
            "0  A person on a horse jumps over a broken down a...   \n",
            "1  A person on a horse jumps over a broken down a...   \n",
            "2  A person on a horse jumps over a broken down a...   \n",
            "3              Children smiling and waving at camera   \n",
            "4              Children smiling and waving at camera   \n",
            "\n",
            "                                          hypothesis  label  \n",
            "0  A person is training his horse for a competition.      1  \n",
            "1      A person is at a diner, ordering an omelette.      2  \n",
            "2                  A person is outdoors, on a horse.      0  \n",
            "3                  They are smiling at their parents      1  \n",
            "4                         There are children present      0  \n",
            "               label\n",
            "count  550152.000000\n",
            "mean        0.996730\n",
            "std         0.819796\n",
            "min        -1.000000\n",
            "25%         0.000000\n",
            "50%         1.000000\n",
            "75%         2.000000\n",
            "max         2.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the label distribution"
      ],
      "metadata": {
        "id": "9k1Dsy0XbC_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = train_data['label'].unique()\n",
        "print(\"Unique label types:\", unique_labels)\n",
        "\n",
        "label_counts = train_data['label'].value_counts()\n",
        "print(\"\\nLabel counts:\")\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn8mm8lBOteD",
        "outputId": "5d95ffc6-f19a-48f7-d79a-5a2a62a409a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label types: [ 1  2  0 -1]\n",
            "\n",
            "Label counts:\n",
            "label\n",
            " 0    183416\n",
            " 2    183187\n",
            " 1    182764\n",
            "-1       785\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assigning number to each triplet"
      ],
      "metadata": {
        "id": "jtL5X427bYo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "triplet_nr = 1\n",
        "triplet_numbers = []\n",
        "\n",
        "# Iterate through the dataset in groups of three rows\n",
        "for i in range(0, len(train_data), 3):\n",
        "    group = train_data.iloc[i:i+3]\n",
        "\n",
        "    # Check if all three rows share the same premise\n",
        "    if len(group['premise'].unique()) == 1 and len(group) == 3:\n",
        "        triplet_numbers.extend([triplet_nr] * 3)\n",
        "        triplet_nr += 1\n",
        "    else:\n",
        "        triplet_numbers.extend([None] * len(group))\n",
        "\n",
        "# Assign triplet numbers to the train_data\n",
        "train_data['triplet_nr'] = triplet_numbers\n",
        "\n",
        "# Drop rows without valid triplet assignment (optional, if you only want valid triplets)\n",
        "train_data = train_data.dropna(subset=['triplet_nr'])\n",
        "\n",
        "# Convert 'triplet_nr' to integer\n",
        "train_data['triplet_nr'] = train_data['triplet_nr'].astype(int)\n",
        "\n",
        "# Print the first few rows\n",
        "print(train_data.head(9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFYJjPhEP_PV",
        "outputId": "32a5f610-f6dd-4e08-da7e-49051862514f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             premise  \\\n",
            "0  A person on a horse jumps over a broken down a...   \n",
            "1  A person on a horse jumps over a broken down a...   \n",
            "2  A person on a horse jumps over a broken down a...   \n",
            "3              Children smiling and waving at camera   \n",
            "4              Children smiling and waving at camera   \n",
            "5              Children smiling and waving at camera   \n",
            "6  A boy is jumping on skateboard in the middle o...   \n",
            "7  A boy is jumping on skateboard in the middle o...   \n",
            "8  A boy is jumping on skateboard in the middle o...   \n",
            "\n",
            "                                          hypothesis  label  triplet_nr  \n",
            "0  A person is training his horse for a competition.      1           1  \n",
            "1      A person is at a diner, ordering an omelette.      2           1  \n",
            "2                  A person is outdoors, on a horse.      0           1  \n",
            "3                  They are smiling at their parents      1           2  \n",
            "4                         There are children present      0           2  \n",
            "5                              The kids are frowning      2           2  \n",
            "6                  The boy skates down the sidewalk.      2           3  \n",
            "7                The boy does a skateboarding trick.      0           3  \n",
            "8               The boy is wearing safety equipment.      1           3  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3942600669fd>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['triplet_nr'] = train_data['triplet_nr'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify triplet numbers containing -1 labels\n",
        "triplets_with_neg1 = train_data[ train_data['label'] == -1]['triplet_nr'].unique()\n",
        "\n",
        "# Step 2: Filter out all rows belonging to those triplets\n",
        "train_data = train_data[~train_data['triplet_nr'].isin(triplets_with_neg1)]\n",
        "\n",
        "# Step 3: Optional - Check the resulting cleaned data\n",
        "print(\"Cleaned data without any triplets containing -1 labels:\")\n",
        "print( train_data.head(9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJbQ0kD9hzAN",
        "outputId": "a3087033-37c4-4774-baba-c23378d366fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data without any triplets containing -1 labels:\n",
            "                                             premise  \\\n",
            "0  A person on a horse jumps over a broken down a...   \n",
            "1  A person on a horse jumps over a broken down a...   \n",
            "2  A person on a horse jumps over a broken down a...   \n",
            "3              Children smiling and waving at camera   \n",
            "4              Children smiling and waving at camera   \n",
            "5              Children smiling and waving at camera   \n",
            "6  A boy is jumping on skateboard in the middle o...   \n",
            "7  A boy is jumping on skateboard in the middle o...   \n",
            "8  A boy is jumping on skateboard in the middle o...   \n",
            "\n",
            "                                          hypothesis  label  triplet_nr  \n",
            "0  A person is training his horse for a competition.      1           1  \n",
            "1      A person is at a diner, ordering an omelette.      2           1  \n",
            "2                  A person is outdoors, on a horse.      0           1  \n",
            "3                  They are smiling at their parents      1           2  \n",
            "4                         There are children present      0           2  \n",
            "5                              The kids are frowning      2           2  \n",
            "6                  The boy skates down the sidewalk.      2           3  \n",
            "7                The boy does a skateboarding trick.      0           3  \n",
            "8               The boy is wearing safety equipment.      1           3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking if the numbers of triplets were assigned correctly to the ones with label -1"
      ],
      "metadata": {
        "id": "7csBvwodbxP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in train_data:\", train_data['label'].unique())\n",
        "\n",
        "triplets_with_neg1 = train_data[train_data['label'] == -1]['triplet_nr'].unique()\n",
        "\n",
        "print(f\"Total number of triplets containing label -1: {len(triplets_with_neg1)}\")\n",
        "triplets_with_neg1_data = train_data[train_data['triplet_nr'].isin(triplets_with_neg1)]\n",
        "\n",
        "print(\"First 3 triplets containing at least one row with label -1:\")\n",
        "for triplet_nr in triplets_with_neg1[:3]:\n",
        "    print(f\"\\nTriplet Number: {triplet_nr}\")\n",
        "    print(triplets_with_neg1_data[triplets_with_neg1_data['triplet_nr'] == triplet_nr])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFItoJi9YdYb",
        "outputId": "9f860c46-33b9-4b3d-968e-ae0b78332ae6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in train_data: [1 2 0]\n",
            "Total number of triplets containing label -1: 0\n",
            "First 3 triplets containing at least one row with label -1:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chosing 100 random triplets containing -1 label\n"
      ],
      "metadata": {
        "id": "AgtJScW4fBWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividing dataset (without triplets with -1 label) in 10 blocks. Choosing 990 random triplets from each block. Combining triplets with and without -1 label."
      ],
      "metadata": {
        "id": "JupRv7jGfWgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "num_blocks = 10\n",
        "triplets_per_block = 1000\n",
        "\n",
        "unique_triplets = train_data['triplet_nr'].unique()\n",
        "triplets_without_neg1 = np.setdiff1d(unique_triplets, triplets_with_neg1)\n",
        "\n",
        "# Calculate the size of each block\n",
        "num_triplets = len(unique_triplets)\n",
        "block_size = num_triplets // num_blocks\n",
        "\n",
        "sampled_triplets = []\n",
        "\n",
        "for i in range(num_blocks):\n",
        "    start_idx = i * block_size\n",
        "    end_idx = (i + 1) * block_size if i < num_blocks - 1 else num_triplets\n",
        "\n",
        "    block_triplets = unique_triplets[start_idx:end_idx]\n",
        "\n",
        "    block_triplets_without_neg1 = np.intersect1d(block_triplets, triplets_without_neg1)\n",
        "\n",
        "    # Sample triplets from the current block\n",
        "    sampled_without_neg1 = train_data[train_data['triplet_nr'].isin(block_triplets_without_neg1)]\n",
        "    sampled_without_neg1 = sampled_without_neg1['triplet_nr'].drop_duplicates().sample(\n",
        "        n=min(triplets_per_block, len(block_triplets_without_neg1)), random_state=42\n",
        "    )\n",
        "    sampled_without_neg1 = train_data[train_data['triplet_nr'].isin(sampled_without_neg1)]\n",
        "    sampled_triplets.append(sampled_without_neg1)\n",
        "\n",
        "# Combine all sampled triplets\n",
        "sampled_triplets_combined = pd.concat(sampled_triplets)\n",
        "\n",
        "# Check the number of unique triplets and label distribution\n",
        "print(f\"Total number of unique triplets chosen: {sampled_triplets_combined['triplet_nr'].nunique()}\")\n",
        "print(\"\\nLabel distribution in the sampled data:\")\n",
        "print(sampled_triplets_combined['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOXjaqfCeB1e",
        "outputId": "31fa8b5e-637b-4388-e19b-492909c822fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique triplets chosen: 10000\n",
            "\n",
            "Label distribution in the sampled data:\n",
            "label\n",
            "0    10071\n",
            "1     9982\n",
            "2     9947\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_triplets_combined.to_csv('sampled_snli_10000.csv', index=False)\n"
      ],
      "metadata": {
        "id": "dIUGB8TJf5H-"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}