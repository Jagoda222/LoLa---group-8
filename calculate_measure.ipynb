{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2wA8lzFbQQYyX/WnzPjVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagoda222/LoLa---group-8/blob/main/calculate_measure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CjUbKdCJnYSu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PNH7S7BMnVXv"
      },
      "outputs": [],
      "source": [
        "def process_snli_dataset_with_measures(file_path, sample_size=700, num_bins=7, measures=None):\n",
        "    \"\"\"\n",
        "    Process an SNLI dataset for curriculum learning with one measure at a time and proportional sampling.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file containing the dataset.\n",
        "        sample_size (int): The number of triplets to sample from the dataset. Default is 700.\n",
        "        num_bins (int): Number of bins for dividing measure values. Default is 7.\n",
        "        measures (list): List of functions to calculate complexity measures.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing two DataFrames (increasing and decreasing) for each measure.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load dataset\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded: {len(data)} rows\")\n",
        "\n",
        "    data['premise'] = data['premise'].fillna(\"\").astype(str)\n",
        "    data['hypothesis'] = data['hypothesis'].fillna(\"\").astype(str)\n",
        "\n",
        "    # Store results\n",
        "    result = {}\n",
        "\n",
        "    # Step 2: Process for each measure separately\n",
        "    for idx, measure_func in enumerate(measures):\n",
        "        measure_name = f\"measure_{idx+1}\"\n",
        "\n",
        "        # Calculate the measure for each row\n",
        "        data[measure_name] = data.apply(measure_func, axis=1)\n",
        "\n",
        "        # Step 3: Calculate triplet-level averages for the current measure\n",
        "        triplet_avg = data.groupby('triplet_nr')[measure_name].mean().reset_index(name='triplet_avg')\n",
        "\n",
        "        # Step 4: Bin triplets based on the current measure's triplet average\n",
        "        bin_edges = np.linspace(triplet_avg['triplet_avg'].min(), triplet_avg['triplet_avg'].max(), num_bins + 1)\n",
        "        triplet_avg['range_bin'] = pd.cut(triplet_avg['triplet_avg'], bins=bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "        # Step 5: Calculate how many triplets to sample from each bin\n",
        "        bin_distribution = triplet_avg['range_bin'].value_counts().sort_index()\n",
        "        print(f\"\\nDistribution of triplets across bins for {measure_name}:\\n{bin_distribution}\")\n",
        "\n",
        "        triplets_per_bin = (bin_distribution / bin_distribution.sum() * sample_size).astype(int)\n",
        "\n",
        "        # Adjust sample size if needed\n",
        "        while triplets_per_bin.sum() < sample_size:\n",
        "            residuals = (bin_distribution / bin_distribution.sum() * sample_size) - triplets_per_bin\n",
        "            triplets_per_bin[residuals.idxmax()] += 1\n",
        "\n",
        "        while triplets_per_bin.sum() > sample_size:\n",
        "            residuals = (bin_distribution / bin_distribution.sum() * sample_size) - triplets_per_bin\n",
        "            triplets_per_bin[residuals.idxmin()] -= 1\n",
        "\n",
        "        # Step 6: Sample triplets proportionally from each bin\n",
        "        sampled_triplets = []\n",
        "        for bin_id, sample_count in triplets_per_bin.items():\n",
        "            if sample_count > 0:\n",
        "                triplets_in_bin = triplet_avg[triplet_avg['range_bin'] == bin_id]['triplet_nr'].values\n",
        "                sampled_triplet_ids = np.random.choice(triplets_in_bin, size=min(sample_count, len(triplets_in_bin)), replace=False)\n",
        "                sampled_triplets.append(data[data['triplet_nr'].isin(sampled_triplet_ids)])\n",
        "\n",
        "        # Combine sampled triplets into a single DataFrame\n",
        "        final_sample = pd.concat(sampled_triplets).reset_index(drop=True)\n",
        "\n",
        "        # Step 7: Merge back the triplet averages\n",
        "        final_sample = final_sample.merge(triplet_avg[['triplet_nr', 'triplet_avg']], on='triplet_nr')\n",
        "\n",
        "        # Step 8: Sort by increasing and decreasing order\n",
        "        final_sample_increasing = final_sample.sort_values(by='triplet_avg').reset_index(drop=True)\n",
        "        final_sample_decreasing = final_sample.sort_values(by='triplet_avg', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        # Store results for this measure\n",
        "        result[measure_name] = {\n",
        "            'increasing': final_sample_increasing,\n",
        "            'decreasing': final_sample_decreasing\n",
        "        }\n",
        "\n",
        "    print(f\"Processed {sample_size} triplets and returned ordered DataFrames for each measure.\")\n",
        "    return result\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Measures\n",
        "def measure_1(row):\n",
        "    \"\"\"Complexity measure: sum of lengths of premise and hypothesis.\"\"\"\n",
        "    return len(row['premise']) + len(row['hypothesis'])\n",
        "\n",
        "def measure_2(row):\n",
        "    \"\"\"Complexity measure: difference in lengths of premise and hypothesis.\"\"\"\n",
        "    return abs(len(row['premise']) - len(row['hypothesis']))\n",
        "\n",
        "def measure_3(row):\n",
        "    \"\"\"Complexity measure: number of unique words in the hypothesis.\"\"\"\n",
        "    return len(set(row['hypothesis'].split()))\n",
        "\n"
      ],
      "metadata": {
        "id": "WVytZqVUTXZM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "file_path = \"/content/sampled_snli_10000.csv\"\n",
        "result = process_snli_dataset_with_measures(file_path, sample_size=700, num_bins=7, measures=[measure_1, measure_2, measure_3])\n",
        "\n",
        "# Step to save increasing and decreasing DataFrames for each measure\n",
        "for measure_name, dataframes in result.items():\n",
        "    # Extract increasing and decreasing DataFrames\n",
        "    increasing_df = dataframes['increasing']\n",
        "    decreasing_df = dataframes['decreasing']\n",
        "\n",
        "    # Save to CSV\n",
        "    increasing_df.to_csv(f\"/content/{measure_name}_increasing.csv\", index=False)\n",
        "    decreasing_df.to_csv(f\"/content/{measure_name}_decreasing.csv\", index=False)\n",
        "\n",
        "    print(f\"Saved {measure_name} DataFrames to CSV:\")\n",
        "    print(f\"/content/{measure_name}_increasing.csv\")\n",
        "    print(f\"/content/{measure_name}_decreasing.csv\")\n"
      ],
      "metadata": {
        "id": "SARFGcpHTZe3",
        "outputId": "836aaf94-6b6f-4bfa-9c38-7e415918df8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 30000 rows\n",
            "\n",
            "Distribution of triplets across bins for measure_1:\n",
            "range_bin\n",
            "0    3856\n",
            "1    5145\n",
            "2     891\n",
            "3      90\n",
            "4      15\n",
            "5       1\n",
            "6       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of triplets across bins for measure_2:\n",
            "range_bin\n",
            "0    8260\n",
            "1    1576\n",
            "2     133\n",
            "3      26\n",
            "4       3\n",
            "6       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of triplets across bins for measure_3:\n",
            "range_bin\n",
            "0     575\n",
            "1    5559\n",
            "2    3162\n",
            "3     606\n",
            "4      85\n",
            "5       9\n",
            "6       4\n",
            "Name: count, dtype: int64\n",
            "Processed 700 triplets and returned ordered DataFrames for each measure.\n",
            "Saved measure_1 DataFrames to CSV:\n",
            "/content/measure_1_increasing.csv\n",
            "/content/measure_1_decreasing.csv\n",
            "Saved measure_2 DataFrames to CSV:\n",
            "/content/measure_2_increasing.csv\n",
            "/content/measure_2_decreasing.csv\n",
            "Saved measure_3 DataFrames to CSV:\n",
            "/content/measure_3_increasing.csv\n",
            "/content/measure_3_decreasing.csv\n"
          ]
        }
      ]
    }
  ]
}