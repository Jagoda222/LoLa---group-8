{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYpOeG6k6I5UTlGHw+WMuu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Dss8Q8yLCwW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from os import path as op\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install syllapy  # Install the syllapy library if needed\n",
        "import syllapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrQaCRSJLOyX",
        "outputId": "943fb6c2-4cf8-453c-a840-4713294bf742"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syllapy\n",
            "  Downloading syllapy-0.7.2-py3-none-any.whl.metadata (854 bytes)\n",
            "Downloading syllapy-0.7.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: syllapy\n",
            "Successfully installed syllapy-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(sentence):\n",
        "        return len(sentence.split())\n",
        "def l_row(row):\n",
        "    word_count = count_words(row['premise']) + count_words(row['hypothesis'])\n",
        "    return word_count\n",
        "\n",
        "def c_row(row, total_rows):\n",
        "    \"\"\"Readability measure using sentence length and word complexity.\"\"\"\n",
        "    avg_sentence_length = l_row(row)/total_rows\n",
        "    avg_word_length = (word_length(row['premise']) + word_length(row['hypothesis'])) / l_row(row)\n",
        "    readability = 0.39 * (avg_sentence_length/ 100) + 11.8 * (avg_word_length / 100) - 15.59\n",
        "    return readability\n",
        "\n",
        "def word_length(sentence):\n",
        "    words = sentence.split()\n",
        "    total_length = sum(len(word) for word in words)\n",
        "    return total_length / len(words) if len(words) > 0 else 0\n",
        "\n",
        "def sentence_length(row): # Change the argument from df to row\n",
        "    \"\"\"sentence length\"\"\"\n",
        "    return l_row(row)\n",
        "\n",
        "def readability(row, total_rows): # Change the argument from df to row\n",
        "    \"\"\"Final measure combining normalized sentence length and readability.\"\"\"\n",
        "    return c_row(row, total_rows)\n",
        "\n",
        "def normalize(series):\n",
        "    \"\"\"Min-max normalization to scale values between 0 and 1.\"\"\"\n",
        "    min_val = np.min(series)\n",
        "    max_val = np.max(series)\n",
        "    return (series - min_val) / (max_val - min_val) if max_val > min_val else series"
      ],
      "metadata": {
        "id": "f4DvsDXSLR2S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-VRI3wqZY2z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_triplets(data, sample_size, num_bins):\n",
        "    # Step 3.1: Divide measure_1 into ranges (bins)\n",
        "    bin_edges = np.linspace(data['measure_1'].min(), data['measure_1'].max(), num_bins + 1)\n",
        "    data['range_bin'] = pd.cut(data['measure_1'], bins=bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "    # Step 3.2: Calculate bin distributions\n",
        "    bin_distribution = data.groupby('range_bin')['triplet_nr'].nunique()\n",
        "    print(f\"Distribution of triplets across bins:\\n{bin_distribution}\")\n",
        "\n",
        "    # Step 3.3: Determine how many triplets to sample from each bin\n",
        "    total_triplets = data['triplet_nr'].nunique()\n",
        "    triplets_per_bin = (bin_distribution / bin_distribution.sum() * 700).astype(int)\n",
        "\n",
        "    # Ensure the sum matches exactly 700\n",
        "    while triplets_per_bin.sum() < 700:\n",
        "        residuals = (bin_distribution / bin_distribution.sum() * 700) - triplets_per_bin\n",
        "        triplets_per_bin[residuals.idxmax()] += 1\n",
        "\n",
        "    while triplets_per_bin.sum() > 700:\n",
        "        residuals = (bin_distribution / bin_distribution.sum() * 700) - triplets_per_bin\n",
        "        triplets_per_bin[residuals.idxmin()] -= 1\n",
        "\n",
        "\n",
        "    # Step 3.4: Sample triplets proportionally from each bin\n",
        "    sampled_triplets = []\n",
        "    for bin_id, sample_count in triplets_per_bin.items():\n",
        "        if sample_count > 0:\n",
        "            triplets_in_bin = data[data['range_bin'] == bin_id]['triplet_nr'].unique()\n",
        "            sampled_triplet_ids = np.random.choice(triplets_in_bin, size=min(sample_count, len(triplets_in_bin)), replace=False)\n",
        "            sampled_triplets.append(data[data['triplet_nr'].isin(sampled_triplet_ids)])\n",
        "\n",
        "    # Combine sampled triplets\n",
        "    sampled_data = pd.concat(sampled_triplets).reset_index(drop=True)\n",
        "\n",
        "    return sampled_data"
      ],
      "metadata": {
        "id": "Fcn9gkFALTfY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_snli_dataset_with_measures(file_path, sample_size=700, num_bins=7):\n",
        "    \"\"\"\n",
        "    Process an SNLI dataset for curriculum learning with nested measures and proportional sampling.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file containing the dataset.\n",
        "        sample_size (int): The number of triplets to sample from the dataset. Default is 700.\n",
        "        num_bins (int): Number of bins for dividing measure_1 values. Default is 7.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Two DataFrames - one ordered by increasing average measure_1 and one by decreasing average measure_1.\n",
        "    \"\"\"\n",
        "    # Step 1:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded: {len(data)} rows\")\n",
        "\n",
        "    data['premise'] = data['premise'].fillna(\"\").astype(str)\n",
        "    data['hypothesis'] = data['hypothesis'].fillna(\"\").astype(str)\n",
        "\n",
        "    print(\"step 1 done\")\n",
        "\n",
        "    # Step 2: CHANGE THE MEASURE\n",
        "    total_rows = len(data)\n",
        "    sentence_l= list(data.apply(sentence_length, axis=1))\n",
        "    read = list(data.apply(lambda row: readability(row, total_rows), axis=1))\n",
        "    # Normalize both measures\n",
        "    normalized_length = normalize(sentence_l)\n",
        "    normalized_readability = normalize( read )\n",
        "\n",
        "    data['measure_1'] = list([l + r for l, r in zip(normalized_length, normalized_readability)])\n",
        "\n",
        "    print(\"step 2 done\")\n",
        "    #Step 3\n",
        "\n",
        "    final_sample = sample_triplets(data, sample_size, num_bins)\n",
        "\n",
        "    print(\"step 3 done\")\n",
        "    # Step 4:\n",
        "    triplet_avg = final_sample.groupby('triplet_nr')['measure_1'].mean().reset_index(name='triplet_avg_measure_1')\n",
        "\n",
        "    # Merge back to keep triplet-level averages\n",
        "    final_sample = final_sample.merge(triplet_avg, on='triplet_nr')\n",
        "    final_sample_increasing = final_sample.sort_values(by='triplet_avg_measure_1').reset_index(drop=True)\n",
        "    final_sample_decreasing = final_sample.sort_values(by='triplet_avg_measure_1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"step 4 done\")\n",
        "    # Print bin distributions for the final samples\n",
        "    increasing_bins = final_sample_increasing.groupby('range_bin')['triplet_nr'].nunique()\n",
        "    print(f\"Distribution of bins in increasing order sample:\\n{increasing_bins}\")\n",
        "    print(f\"Sampled {sample_size} triplets and returned two ordered DataFrames.\")\n",
        "\n",
        "    return final_sample_increasing, final_sample_decreasing\n"
      ],
      "metadata": {
        "id": "ZSsUmaAoLWx5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function with the basic measure\n",
        "sample_increasing, sample_decreasing = process_snli_dataset_with_measures(file_path='/content/sampled_snli_10000.csv')\n",
        "\n",
        "# Display the outputs\n",
        "print(\"Sample ordered by increasing triplet average:\")\n",
        "print(sample_increasing.head())\n",
        "\n",
        "#print(\"\\nSample ordered by decreasing triplet average:\")\n",
        "#print(sample_decreasing.head())\n",
        "sample_increasing.to_csv('sample_increasing.csv', index=False)\n",
        "#sample_decreasing.to_csv('sample_decreasing.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnWmv6lLLdyU",
        "outputId": "4ec969a6-ccfb-4d1e-cf49-c76dee82e19f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 30000 rows\n",
            "step 1 done\n",
            "step 2 done\n",
            "Distribution of triplets across bins:\n",
            "range_bin\n",
            "0    8031\n",
            "1    4275\n",
            "2     429\n",
            "3      62\n",
            "4      10\n",
            "5       3\n",
            "6       3\n",
            "Name: triplet_nr, dtype: int64\n",
            "step 3 done\n",
            "step 4 done\n",
            "Distribution of bins in increasing order sample:\n",
            "range_bin\n",
            "0    562\n",
            "1    385\n",
            "2     45\n",
            "3      5\n",
            "4      1\n",
            "Name: triplet_nr, dtype: int64\n",
            "Sampled 700 triplets and returned two ordered DataFrames.\n",
            "Sample ordered by increasing triplet average:\n",
            "                           premise                         hypothesis  label  \\\n",
            "0  A girl rows a boat from a dock.          A boy plays on a jet ski.      2   \n",
            "1  A girl rows a boat from a dock.               A girl in a rowboat.      0   \n",
            "2  A girl rows a boat from a dock.  A girl rows out to meet a friend.      1   \n",
            "3              A boy on a scooter.               The boy is on a bike      2   \n",
            "4              A boy on a scooter.                     A boy outside.      0   \n",
            "\n",
            "   triplet_nr  measure_1  range_bin  triplet_avg_measure_1  \n",
            "0      148994   0.197785          0               0.203772  \n",
            "1      148994   0.201260          0               0.203772  \n",
            "2      148994   0.212272          0               0.203772  \n",
            "3        7165   0.184223          0               0.215421  \n",
            "4        7165   0.254448          0               0.215421  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uH6cN4CKMExm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}