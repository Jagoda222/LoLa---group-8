{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1BpdexuTy6/vi9YyntNvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagoda222/LoLa---group-8/blob/main/run_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.9.0"
      ],
      "metadata": {
        "id": "cRdJsHcDOvd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2f84cf-bb6f-4bec-ab01-218dad9ea742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.9.0 in /usr/local/lib/python3.11/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.9.0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (24.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.9.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.9.0) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.9.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.9.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.9.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.9.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.9.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDBzyaAdOndE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# META Variables\n",
        "# it is good to have certain directories for saving model checkpoints (e.g., on google drive)\n",
        "MODEL_DIR = 'model_checkpoints'\n",
        "MODEL_CHECKPOINT = \"microsoft/deberta-v3-small\"\n",
        "BATCH_SIZE = 16\n",
        "DATA_DIRECTORY = 'data/data_samples'\n",
        "\n",
        "\"\"\" Change the name of the DATA_DIRECTORY to the folder where your orderings are:\n",
        "it expects the data in the following format :\n",
        "\n",
        "premise hypothesis label\n",
        "...      ...        ...\n",
        "\n",
        "The code automatically reverses the order of the data, and calculates the accuracy for this ordering.\n",
        "It also does the random shuffle for the baseline measurement (and calculates accuracy of course).\n",
        "\n",
        "The output will be a file of the format:\n",
        "\n",
        "name_of_metric  baseline    curriculum  anti-curriculum\n",
        "\n",
        "sentence_length     0.84           0.82            0.80\n",
        "etc.\n",
        "\"\"\"\n",
        "\n",
        "# SNLI data needs to be cleaned as it contains -1s as a label\n",
        "#for k in snli_data:\n",
        "    #snli_data[k] = snli_data[k].filter( lambda prob: prob['label'] >= 0 )\n",
        "\n",
        "\n",
        "# Define the column names\n",
        "columns = ['ordering', 'baseline', 'curriculum', 'anti-curriculum']\n",
        "# Create an empty DataFrame with the specified columns\n",
        "out_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "metric = load_metric('glue', \"mnli\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "\n",
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "def preprocess_function(d):\n",
        "    return tokenizer(d['premise'], d['hypothesis'], truncation=True)\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=4)\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "# validation gets encoded outside the loop\n",
        "snli_validation = load_dataset(\"snli\", split=\"validation\")\n",
        "encoded_snli_validation = snli_validation.map(preprocess_function, batched=True, load_from_cache_file=True)\n",
        "\n",
        "data_files = [f for f in os.listdir(DATA_DIRECTORY) if f.endswith('.csv')]\n",
        "\n",
        "for file_name in data_files:\n",
        "\n",
        "    file_path = os.path.join(DATA_DIRECTORY, file_name )\n",
        "    snli_data = load_dataset(\"csv\", data_files=file_path)\n",
        "\n",
        "    snli_data = snli_data[\"train\"].select_columns([\"premise\", \"hypothesis\", \"label\"])\n",
        "    snli_data_reversed = snli_data.select(range(len(snli_data)-1, -1, -1))\n",
        "    snli_data_random_shuffle = snli_data.shuffle(seed=100)\n",
        "\n",
        "\n",
        "    new_row = [file_name]\n",
        "\n",
        "    for data in [snli_data_random_shuffle, snli_data, snli_data_reversed]:\n",
        "\n",
        "        encoded_snli_data = data.map(preprocess_function, batched=True, load_from_cache_file=True)\n",
        "\n",
        "        #instantiate the model\n",
        "        args = TrainingArguments(\n",
        "        output_dir=MODEL_DIR,             # Directory to save model checkpoints\n",
        "        evaluation_strategy=\"steps\",      # Evaluates the model at regular intervals (defined by eval_steps)\n",
        "        eval_steps=200,                   # Number of steps between evaluations\n",
        "        save_steps=200,                   # Number of steps between saving checkpoints\n",
        "        logging_steps=100,                # Number of steps between logging metrics\n",
        "        per_device_train_batch_size=16, # Number of samples per training batch per device\n",
        "        per_device_eval_batch_size=16,  # Number of samples per evaluation batch per device\n",
        "        learning_rate=5.1e-05,               # Initial learning rate for the optimizer\n",
        "        num_train_epochs= 4,               # Total number of training epochs\n",
        "        weight_decay=0.0074,                # L2 weight regularization to prevent overfitting\n",
        "        warmup_steps = 211,                 # Number of warmup steps for the learning rate scheduler\n",
        "        save_total_limit=2,               # Limits the number of saved checkpoints to save disk space\n",
        "        load_best_model_at_end=True,      # Automatically loads the best checkpoint after training\n",
        "        metric_for_best_model=\"accuracy\", # Metric used to determine the best model during evaluation\n",
        "        greater_is_better=True,           # Indicates whether a higher metric value is better\n",
        "        logging_dir=\"./logs\",             # Directory to save TensorBoard logs\n",
        "        fp16=True,                        # Enables mixed precision training for faster computation\n",
        "        lr_scheduler_type= \"cosine\",       # Uses cosine learning rate decay for smoother transitions\n",
        "\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model,\n",
        "            args,\n",
        "            train_dataset=encoded_snli_data,\n",
        "            eval_dataset=encoded_snli_validation,\n",
        "            # You could use \"test\" here but it will be cheating then\n",
        "            # to select the model checkpoint which gets highest score on test\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "            )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "\n",
        "        # Evaluate the model on validation set\n",
        "        eval_results = trainer.evaluate()\n",
        "        new_row.append(eval_results[\"eval_accuracy\"])\n",
        "        eval_results\n",
        "\n",
        "    # append the evaluations to the table\n",
        "    out_df.loc[len(out_df)] = new_row\n",
        "\n",
        "out_df.to_csv(\"evaluations.csv\", index=False)"
      ],
      "metadata": {
        "id": "zd8OTu2dOqPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}