{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagoda222/LoLa---group-8/blob/main/LoLa_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools for the Logic & Language course\n",
        "\n",
        "by Lasha.Abzianidze@gmail.com"
      ],
      "metadata": {
        "id": "Lh_nsRQ655F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaCy processing"
      ],
      "metadata": {
        "id": "IYNJMnO72b3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "print(f\"spaCy version={spacy.__version__}\")"
      ],
      "metadata": {
        "id": "Jrd_Rb0n2dD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading spaCy's medium/large model if needed (small is downloaded by default)\n",
        "# !python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "mpnCF3cG2k-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLP = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "WcHmwdNy2x2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"This is a sample sentence to be parsed\"\n",
        "doc = NLP(sent)"
      ],
      "metadata": {
        "id": "kTOuiFO82zu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.displacy.render(doc, style='dep', jupyter=True, options={'fine_grained':True, 'compact':False})"
      ],
      "metadata": {
        "id": "KYHdSgac3IYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if many sentences needs to be parsed, use pipe\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "docs_sm = list(NLP.pipe(1000 * [sent]))\n",
        "print(len(docs_sm))"
      ],
      "metadata": {
        "id": "HqdcpQpGukYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CoreNLP parsing"
      ],
      "metadata": {
        "id": "4XbOrPEC3wj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoreNLP will be used through [Stanza CoreNLP interface](https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb). CoreNLP provides both constituency and dependency trees. For English, it is possible to directly get dependency trees with a dependency parser or indirectly obtain them by converting the constituency trees into dependecy trees."
      ],
      "metadata": {
        "id": "wSzX73d84XX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "XOByjVMA4oDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import os\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "# Import client module\n",
        "from stanza.server import CoreNLPClient\n",
        "# src: https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb\n",
        "from nltk.tree import Tree"
      ],
      "metadata": {
        "id": "SFPFfBEL4g7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency parsing"
      ],
      "metadata": {
        "id": "oG2fAnOg4-70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents = [\"This is a sample sentence to be parsed\", \"A brown fox is jumping over the lazy dog\"]"
      ],
      "metadata": {
        "id": "64cufKMt5OL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dependency trees from a dependency parser\n",
        "# https://stanfordnlp.github.io/CoreNLP/depparse.html\n",
        "with CoreNLPClient(annotators='tokenize,pos,depparse',\n",
        "                   memory='4G', endpoint='http://localhost:9021', be_quiet=True,\n",
        "                   output_format='json') as client:\n",
        "    core_dep_parses = [ client.annotate(s)['sentences'][0] for s in sents ]"
      ],
      "metadata": {
        "id": "MtH9IG39p4jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constituency parsing"
      ],
      "metadata": {
        "id": "Z41CZjP569Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dependency trees from a constituency parser\n",
        "# takes 3-4min\n",
        "# https://stanfordnlp.github.io/CoreNLP/parse.html\n",
        "with CoreNLPClient(annotators='tokenize,pos,parse',\n",
        "                   memory='4G', endpoint='http://localhost:9030', be_quiet=True,\n",
        "                   output_format='json') as client:\n",
        "    core_con_parses = [ client.annotate(s)['sentences'][0] for s in sents ]"
      ],
      "metadata": {
        "id": "ttK7fSg4LFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drawing CoreNLP constituency trees with NLTK's Tree object\n",
        "Tree.fromstring(core_con_parses[0]['parse']).pretty_print()"
      ],
      "metadata": {
        "id": "slfJfHDKoi98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AMuSE word senses"
      ],
      "metadata": {
        "id": "nylSkAwH9kcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use an API to predict word senses with the help of the multilingual word sense disambiguation system. For more details visit [here](http://nlp.uniroma1.it/amuse-wsd/about).  "
      ],
      "metadata": {
        "id": "HK2B89Es-pla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "0mr7HqaJ9qZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n",
        "url = 'http://nlp.uniroma1.it/amuse-wsd/api/model'"
      ],
      "metadata": {
        "id": "85Xnmvkd9u9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disambiguation ENglish sentences\n",
        "input = [\n",
        "    {'text': \"This table is too long for this room\", \"lang\": \"EN\" },\n",
        "    {'text': \"The bank is wet\", \"lang\": \"EN\" }\n",
        "]"
      ],
      "metadata": {
        "id": "OY0Jxk-J9yj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.post(url, json=input, headers=headers).json()"
      ],
      "metadata": {
        "id": "oZXlGFmJ-JWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "-W-MwHMN-R3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prover9\n",
        "\n",
        "The NLTK-native tableau prover for FOL cannot handle the equality predicate properly. That's why we will use Prover9, a proper theorem prover from FOL. Fortunately, it is nicely integrated in NLTK classes.        \n",
        "We need to download Prover9 as it is not by default available in recent NLTK anymore."
      ],
      "metadata": {
        "id": "_rsLi3ahsUlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.__version__)"
      ],
      "metadata": {
        "id": "xnGJ1976wfJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "prover9_file_name=\"p9m4-v05.tar.gz\"\n",
        "[[ ${prover9_file_name} =~ (.+)\\.tar\\.gz ]]\n",
        "prover9_folder_name=${BASH_REMATCH[1]}\n",
        "if [[ ! -d ${prover9_folder_name} ]]; then\n",
        "  curl -sL \"https://www.cs.unm.edu/~mccune/prover9/gui/$prover9_file_name\" -o ${prover9_file_name}\n",
        "  tar -xzf ${prover9_file_name}\n",
        "  rm -rf 'prover9'\n",
        "  mv ${prover9_folder_name} 'prover9'\n",
        "  rm ${prover9_file_name}\n",
        "fi"
      ],
      "metadata": {
        "id": "KNi_H8RgsUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prover9 = nltk.Prover9()\n",
        "prover9.config_prover9(\"/content/prover9/bin\")"
      ],
      "metadata": {
        "id": "2jpZNJSvvzR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str2exp = nltk.sem.Expression.fromstring"
      ],
      "metadata": {
        "id": "cPqZmT-PwlZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premises = [\"all x.(man(x) -> walks(x))\", \"not walks(Alex)\"]\n",
        "conclusion = \"some y. not man(y)\"\n",
        "prover9.prove(str2exp(conclusion), [ str2exp(p) for p in premises ])"
      ],
      "metadata": {
        "id": "iuoZR38HiGid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conclusion = \"exists x. (L(x) & exists y. (E(y) & y = x)) -> exists x.(L(x) & E(x))\"\n",
        "prover9.prove(str2exp(conclusion), [])"
      ],
      "metadata": {
        "id": "Lg53aJK6xF_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CFG parsing and generation"
      ],
      "metadata": {
        "id": "hsNKszvOxxXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.__version__)"
      ],
      "metadata": {
        "id": "wnl6rOJsx052"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to graphically display parse trees (when ascii display is not enough)\n",
        "!pip install svgling\n",
        "import svgling"
      ],
      "metadata": {
        "id": "ZzlIGYduyggn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing with CFG"
      ],
      "metadata": {
        "id": "SQY04thayEyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = nltk.ChartParser(groucho_grammar)\n",
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    PP -> P NP\n",
        "    NP -> Det N | Det N PP | 'I'\n",
        "    VP -> V NP | VP PP\n",
        "    Det -> 'an' | 'my'\n",
        "    N -> 'elephant' | 'pajamas'\n",
        "    V -> 'shot'\n",
        "    P -> 'in'\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "2sImTWXjx57i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'I shot an elephant in my pajamas'.split()"
      ],
      "metadata": {
        "id": "1KDT8jczx8Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = nltk.ChartParser(groucho_grammar)"
      ],
      "metadata": {
        "id": "ofMy4Dpwx99c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tree in parser.parse(sent):\n",
        "    print(tree)"
      ],
      "metadata": {
        "id": "7v6MwnzoyOjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively you can print trees in a more beautiful way\n",
        "for tree in parser.parse(sent):\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "id": "5O12c5aSU8XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graphical display of the tree\n",
        "tree"
      ],
      "metadata": {
        "id": "-Z4c833JyWkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating with CFG\n",
        "\n",
        "We can also generate sentences from a CFG. All generated sentences are automatically parsable by the grammar that generated them. Sometimes grammars can generate infinitely many sentences. For this, one needs to limit the output of the generation, otherwise the code won't terminate.  \n",
        "Read here more about the generation: [howto](https://www.nltk.org/howto/generate.html), [source](https://www.nltk.org/_modules/nltk/parse/generate.html)."
      ],
      "metadata": {
        "id": "BtmvlZ2q3aEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse.generate import generate"
      ],
      "metadata": {
        "id": "9ES0CQmT7YoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's limit generation with number of trees (10). It also supports max depth constraint.\n",
        "# Note that generated sentecnes are a list of words\n",
        "for sent in generate(groucho_grammar, n=20, depth=4):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "Xpps1qUt7MTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing with feature-based CFG"
      ],
      "metadata": {
        "id": "tFDA8AozXZss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section largely follows [Chapter 9](https://www.nltk.org/book/ch09.html), but also digs into several issues that are left unxplained in the NLTK book.  \n",
        "[This page](https://www.nltk.org/howto/featgram.html) also provides examples of usage of the feature-based grammars, but instructions might seem relatively terse.  \n",
        "Note that Section 3 in Chpater 9 goes too deep into a feature-based grammar and syntactic theory. Feel free to read it if you find it interesting but expect to find some parts unclear or shallow. Also it is useful to know what are the limits of the implementation of feature-based grammars in NLTK."
      ],
      "metadata": {
        "id": "m69BocoeX0mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('book_grammars') # download predefined grammars\n",
        "from nltk.grammar import FeatureGrammar\n",
        "from nltk.parse.featurechart import FeatureChart, FeatureChartParser\n",
        "from nltk.featstruct import Feature\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "8JJYfH47Xg-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The grammar is taken from grammars/book_grammars/feat0.fcfg\n",
        "GR1 = \"\"\"\n",
        "% start S\n",
        "# ###################\n",
        "# Grammar Productions\n",
        "# ###################\n",
        "# S expansion productions\n",
        "S -> NP[NUM=?n] VP[NUM=?n]\n",
        "# NP expansion productions\n",
        "NP[NUM=?n] -> N[NUM=?n]\n",
        "NP[NUM=?n] -> PropN[NUM=?n]\n",
        "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
        "NP[NUM=pl] -> N[NUM=pl]\n",
        "# VP expansion productions\n",
        "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
        "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
        "# ###################\n",
        "# Lexical Productions\n",
        "# ###################\n",
        "Det[NUM=sg] -> 'this' | 'every'\n",
        "Det[NUM=pl] -> 'these' | 'all'\n",
        "Det -> 'the' | 'some' | 'several'\n",
        "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
        "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
        "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\n",
        "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
        "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
        "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
        "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
        "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
        "TV[TENSE=past] -> 'saw' | 'liked'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T3kcoaYW1l5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a grammar object from the string\n",
        "gr1 = FeatureGrammar.fromstring(GR1)\n",
        "print(f\"The type of gr1 is {type(gr1)}\")"
      ],
      "metadata": {
        "id": "sKYtzDLTcP65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLTK, featured structures are a set of attribute-value pairs, but they also have a non-terminal symbol, e.g., `S`, `NP`, `VP`, etc. What are these symbols in feature structures? Each feature structure has a special feature called `type` and its values are these symbols. Let's get the start symbol of the grammar."
      ],
      "metadata": {
        "id": "Nm05L3PqfX5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The start symbol of gr1 in a raw format is {gr1.start()}\")\n",
        "print(f\"The start symbol of gr1 is a clean format is {gr1.start()[Feature('type')]}\")"
      ],
      "metadata": {
        "id": "sqUYw3s3gZM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a parser based on the grammar object. The parser we create is a chart parser (you have seen a `chart` parser in the previous section too). You can read more about chart parsing in [J&M (Ch13-13.2)](https://web.stanford.edu/~jurafsky/slp3/13.pdf) or [here](https://en.wikipedia.org/wiki/CYK_algorithm), but it is not necessary as such at this stage."
      ],
      "metadata": {
        "id": "1IixILtmgzDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a chart parser object based on the grammar\n",
        "parser1 = FeatureChartParser(gr1)\n",
        "# this is same as\n",
        "# FeatureChartParser(gr1, trace=0, chart_class=FeatureChart)\n",
        "# When trace is > 0, the parsing procedure prints the workings of the parsing algorithm\n",
        "# this is useful when the parser produces unexpected results"
      ],
      "metadata": {
        "id": "lhDItSDPch6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's parse some grammatical and ungrammatical sentences."
      ],
      "metadata": {
        "id": "olM5teCqi9EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tree in parser1.parse(\"this dog likes children\".split()):\n",
        "    print(tree) # plain print, unfortunately featured grammar trees are not supporting pretty_print yet in NLTK"
      ],
      "metadata": {
        "id": "9iM3NrJZiAaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *this dogs: det-noun number disagreemnet\n",
        "for tree in parser1.parse(\"this dogs likes children\".split()):\n",
        "    print(tree) # plain print, unfortunately featured grammar trees are not supporting pretty_print yet in NLTK\n",
        "\n",
        "# *dogs likes: subject-verb number disagreement\n",
        "for tree in parser1.parse(\"this dogs likes children\".split()):\n",
        "    print(tree) # plain print, unfortunately featured grammar trees are not supporting pretty_print yet in NLTK"
      ],
      "metadata": {
        "id": "QgN-oRX2iHWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code prints nothing as no parses are found for the ungrammatical sentences, i.e., the iterator the parser returns is empty, hence the body of the for-loop is not carried out.\n",
        "\n",
        "Now, this is a significant improvement over the previous bare, feature-less, grammar. But the grammar cannot distinguish semantically nonsensical sentences from sensible ones (e.g., if the grammar had appropriate rules and descriptions for certain words, it would parse the famous sentence [Colorless green ideas sleep furiously](https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously) ). It would be too much to ask such fine-grained distinction from the syntax-based grammar."
      ],
      "metadata": {
        "id": "f2JjLFk0jbm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating with feature-based CFG"
      ],
      "metadata": {
        "id": "hkYnA5942-M6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately when using the featured CFG, the generation ignores constraints from the features and generates more sentences than it should.  \n",
        "Here is my NLTK [issue](https://github.com/nltk/nltk/issues/2628) about this (I don't think it is fixed).  \n",
        "The issue also mentions a workaround, which is to generate sentences from FCFG and then keep only those sentences that are parsed by the same FCFG."
      ],
      "metadata": {
        "id": "PxcGFEGM8FzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a BERT-like model on NLI"
      ],
      "metadata": {
        "id": "RAtLN8YesLBt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa07FK9Asqmo"
      },
      "source": [
        "## Loading data and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxsfOh8UnVqh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from os import path as op\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HlzJe4sq9rB"
      },
      "outputs": [],
      "source": [
        "# Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc24bjmHnbPA"
      },
      "outputs": [],
      "source": [
        "# transformers complained about newset version 0.0.13 so installing the older version\n",
        "# ! pip install huggingface-hub==0.0.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZl7hukGnnub"
      },
      "outputs": [],
      "source": [
        "! pip install datasets #transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibI9uDj8wKTs"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3X3wzNonwTF"
      },
      "outputs": [],
      "source": [
        "# META Variables\n",
        "# it is good to have certain directories for saving model checkpoints (e.g., on google drive)\n",
        "MODEL_DIR = 'model_checkpoints'\n",
        "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xaEJzOjw3Bm"
      },
      "outputs": [],
      "source": [
        "snli_data = load_dataset(\"snli\")\n",
        "print(Counter(snli_data['train']['label']))\n",
        "\n",
        "# SNLI data needs to be cleaned as it contains -1s as a label\n",
        "for k in snli_data:\n",
        "    snli_data[k] = snli_data[k].filter( lambda prob: prob['label'] >= 0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqrUc0KPoDJ1"
      },
      "outputs": [],
      "source": [
        "metric = load_metric('glue', \"mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH4CBGqfoH2X"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYa1zaH8oKQw"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/transformers/preprocessing.html\n",
        "def preprocess_function(d):\n",
        "    return tokenizer(d['premise'], d['hypothesis'], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCwkBU1MoOIu"
      },
      "outputs": [],
      "source": [
        "# tokenize the data\n",
        "encoded_snli_data = snli_data.map(preprocess_function, batched=True, load_from_cache_file=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ29yw1woSR6"
      },
      "outputs": [],
      "source": [
        "# load a model and prepare it for 3-way classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdimvb5JIzxw"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJAga1nWob0R"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cCi-3EmoVG1"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    MODEL_DIR, # to save models\n",
        "    # evaluation_strategy = \"epoch\", # 1 epoch for training takes too long for colab\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 500, # evaluate and save after training on every next 500x16 examples\n",
        "    save_steps=500, # saves model after every 500 steps. save_steps should be divisible on eval_steps\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=1, # going throught the training data only once\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True, # after fine-tuning trainer.model will keep the best model\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo4L9DUrodmC"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_snli_data[\"train\"],\n",
        "    eval_dataset=encoded_snli_data[\"validation\"],\n",
        "    # You could use \"test\" here but it will be cheating then\n",
        "    # to select the model checkpoint which gets highest score on test\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGXmC_fSbDWV"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "# it takes ~32min to fine-tune one epoch on the training set (550K problems) on V100\n",
        "# it takes ~45min to fine-tune one epoch on the training set (550K problems) on T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcF9Dn9Ax0UA"
      },
      "outputs": [],
      "source": [
        "# if colab timeouts after one evaluation (i.e., training on 5000x16),\n",
        "# you will still have a model in $MODEL_DIR/checkpoint-5000\n",
        "# you can load that model and continue fine-tuning on the remaining problems\n",
        "# note that the first 5000x16 problems will be skipped\n",
        "trainer.train(op.jopin(MODEL_DIR, 'checkpoint-5000'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pui0BubwI70k"
      },
      "source": [
        "## Evaluation (no fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNwqSS3Nyllm"
      },
      "outputs": [],
      "source": [
        "# evaluation of a particular model\n",
        "\n",
        "# if you want to load a model from a checkpoint for evaluation\n",
        "# ft_model = AutoModelForSequenceClassification.from_pretrained(op.join(MODEL_DIR, 'checkpoint-5000'))\n",
        "\n",
        "trainer_eval = Trainer(\n",
        "    trainer.model, # model that you want to evaluate, In this case this is the best model based on the fine-tuning\n",
        "    args,\n",
        "    train_dataset=encoded_snli_data[\"train\"],\n",
        "    eval_dataset=encoded_snli_data[\"validation\"], # you want to evaluate on test\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_eval.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision trees on SNLI"
      ],
      "metadata": {
        "id": "qy4f-6yLlz63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code examples below show you how to use `snli_jsonl2dict` function to read data from SNLI files. The read data separates NLI problem info from sentence annotations because many sentences occur in many NLI problems and there is no need to reprocess the same sentences every time it is encountered in an NLI problem. This separation saves space and runtime, and it does make difference when you think of creating feature representations of 550K NLI problems.  \n",
        "\n",
        "You are also provided with `sen2features`, `problem2features`, and `probs2df` functions to show you how feature selection on a sentence level and a problem level can be done in a modular way. Note that the provided features are very simplistic ones. Try to replace them with more effective or reasonable ones. The final function demostrates how to visually verify/view feature representation of the problems (the latter is useful to verify whether your code is really doing what you think it should be doing)."
      ],
      "metadata": {
        "id": "imB1E9tFlz67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bI6hXUuexfwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigntools package is a course specific collection of useful tools\n",
        "!rm -fr assigntools # helps to rerun this cell witthout errors, if recloning needed\n",
        "! git clone https://github.com/kovvalsky/assigntools.git"
      ],
      "metadata": {
        "id": "aCZo1GENv1gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from assigntools.LoLa.read_nli import snli_jsonl2dict, sen2anno_from_nli_problems\n",
        "from assigntools.LoLa.sen_analysis import spacy_process_sen2tok, display_doc_dep"
      ],
      "metadata": {
        "id": "xHNHqdmqvu1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading data"
      ],
      "metadata": {
        "id": "62OYO9JIPlgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tree import Tree"
      ],
      "metadata": {
        "id": "LvOs4JA-3k17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SNLI data on fly\n",
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip\n",
        "# !rm -r __MACOSX/ snli_1.0/*_test*"
      ],
      "metadata": {
        "id": "5lKU860ih-Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes ~1min to read and pre-process data\n",
        "# By default it reads the problems that have a gold label.\n",
        "# SNLI is dict {part: {problem_id: problem_info}}\n",
        "# S2A is dict {sentence: sentence annotation dict}\n",
        "SNLI, S2A = snli_jsonl2dict('snli_1.0')"
      ],
      "metadata": {
        "id": "dK0AXvsgfltO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access a problem with its ID in the train part\n",
        "some_prob = SNLI['train']['4804607632.jpg#0r1e']\n",
        "display(some_prob) # you can use print but the data will be squeezed in a single line"
      ],
      "metadata": {
        "id": "SM8eR682Iu23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The analysis/annotation of the hypothesis sentence\n",
        "# It includes tokenization, tree structures and pos tags.\n",
        "# Additionally, for each sentence you can find out in which\n",
        "# parts, problems, and role (premise or hypothesis) it occurs.\n",
        "# Check the key \"pids\" (problemIDs) for this info.\n",
        "print(f\"Sentence: {some_prob['h']}\")\n",
        "S2A[some_prob['h']]"
      ],
      "metadata": {
        "id": "vnnP33PJJRHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is a good idea to keep the problem annotations and sentence annotations separately\n",
        "# because many sentences occur in many NLI problems and you don't want to extract features\n",
        "# for the same sentence for each problem it occurs in.\n",
        "# For example the following sentence occurs many times in NLI problems\n",
        "len(S2A[\"A man is sleeping.\"]['pids'])"
      ],
      "metadata": {
        "id": "JPH1SudlFOBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying syntax trees"
      ],
      "metadata": {
        "id": "rQrkjDep4xma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can read tree representations as NLTK Tree objects\n",
        "t = Tree.fromstring(S2A[some_prob['h']]['tree'])\n",
        "print(t)\n",
        "# better printing\n",
        "t.pretty_print()"
      ],
      "metadata": {
        "id": "f7LvFC6D3WZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to have svgline installed to display tree objects\n",
        "! pip install svgling"
      ],
      "metadata": {
        "id": "_8kVznHZ4VXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display tree\n",
        "t"
      ],
      "metadata": {
        "id": "TJnXUvyj4inJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing with spaCy [optional]"
      ],
      "metadata": {
        "id": "GrXD-I5z1L1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more reasoning-relevant features, one can use [spaCy](https://spacy.io/) to get dependency parse trees for sentences. In addition to the dependency parsing, spaCy pipeline also does part-of-speech tagging (with general and fine-grained POS tags), named entity recognition, and lemmatization (details [here](https://spacy.io/usage/processing-pipelines)). For a quick intro to spaCy, have a look at the following section in the [spaCy tutorial](https://course.spacy.io/en/): sections 1 & 5 in [chapter 1](https://course.spacy.io/en/chapter1), and 4 & 8 in [chapter 2](https://course.spacy.io/en/chapter2).   \n",
        "Use attributes of spaCy's [Token objects](https://spacy.io/api/token).  \n",
        "After annotation, tokens come with two pos tags: fine-grained corresponds to [Penn Treebank pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) while coarse-grained to [Universal pos tags](https://universaldependencies.org/u/pos/). The dependency parse trees follow the [stanford style](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf).  "
      ],
      "metadata": {
        "id": "ajg-s4mf1ZM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading spaCy's large model\n",
        "# !python -m spacy download en_core_web_lg\n",
        "import spacy"
      ],
      "metadata": {
        "id": "W6bY0Emz3KO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLP = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "t8R-birU3bJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SNLI train part contains 640K different sentences\n",
        "# First, processing all these sentences with spaCy and then using the analyses\n",
        "# for feature extraction is not feasible as the colab will run out of the memory\n",
        "# There are two options, either reduce the number of sentecnes by using subpart\n",
        "# of the training part, or process the sentences with spaCy in batches while\n",
        "# at the same time converting NLI problems into a set of feature-values\n",
        "# The former is simpler and this is how you can create new SNLI and S2A variables\n",
        "print(f\"Train contains {len(SNLI['train'])} problems\")\n",
        "print(f\"The number of different sentences in SNLI: {len(S2A)}\")\n",
        "# Let's decide that we take first 10K problems from TRAIN\n",
        "# (the label distribution should reflect the original distribution from the training data)\n",
        "SNLI['sub_train'] = { pid: SNLI['train'][pid] for pid in sorted(SNLI['train'])[:10000] }\n",
        "sub_S2A = sen2anno_from_nli_problems({**SNLI['sub_train'], **SNLI['dev']}, S2A)\n",
        "print(f\"The number of different sentences in subTRAIN and DEV: {len(sub_S2A)}\")"
      ],
      "metadata": {
        "id": "T4J7GVJeWDCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process all sentecnes in DEV and subTRAIN with spaCy\n",
        "# Note thet the following function takes spaCy pipeline and sen->tokens dict\n",
        "# With the tokenization input, the pipeline is forced to use the same tokenization\n",
        "sen2Doc = spacy_process_sen2tok(NLP, { sen: anno['tok'] for sen, anno in sub_S2A.items() })"
      ],
      "metadata": {
        "id": "EHU6OGIlfinH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_doc_dep(sen2Doc[\"A man is sleeping.\"])"
      ],
      "metadata": {
        "id": "QUiqLx3ul8Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create features [demo]"
      ],
      "metadata": {
        "id": "ZiAkHr7-Pfnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section shows one way how you could organize your code in a modular and hierarchical way: separate sentence-level features from problem/pair-level features where the latter uses the former. The conversion of the entire training data into feature-values is wrapped in a separate function so that it can work for train, dev and test parts in the similar way.  "
      ],
      "metadata": {
        "id": "wMZA1J4mH6kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the function\n",
        "def sen2features(sen, anno):\n",
        "    '''\n",
        "    Takes a sentence and its annotation and returns a dictionary\n",
        "    of feature:value that characterizes the sentence\n",
        "    '''\n",
        "    feats = {}\n",
        "    # number of tokens\n",
        "    feats['tok_num'] = len(anno['tok'])\n",
        "    # number of negation words\n",
        "    neg_words = \"n't no not\"\n",
        "    feats['neg_num'] = len([ t for t in anno['tok'] if t.lower() in neg_words ])\n",
        "    # number of nouns\n",
        "    feats['noun_num'] = len([ t for t in anno['pos'] if t == \"NNS\" or t == \"NN\" ])\n",
        "\n",
        "    return {**feats, **anno}"
      ],
      "metadata": {
        "id": "NjzKBnZ7UrRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add features to the sentence annotations\n",
        "s2af = { s: sen2features(s, a) for s, a in tqdm(sub_S2A.items()) }"
      ],
      "metadata": {
        "id": "vLIGFVRPXxFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of a sentence with feature-added annotations\n",
        "s2af['The women is not on her phone.']"
      ],
      "metadata": {
        "id": "AMht8gs4YOII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the function\n",
        "def problem2features(sen1, anno1, sen2, anno2, sen_feats=set(['tok_num', 'neg_num', 'noun_num'])):\n",
        "    '''\n",
        "    Takes two sentences and their anotations (_features) and returns a dictionary of\n",
        "    feature:value that characterizes the sentence pair, i.e. feature is about both sentences\n",
        "    '''\n",
        "    features = {}\n",
        "    # define the sentence-based features that will be part of the problem features\n",
        "    sen_feats = set('tok_num neg_num noun_num'.split())\n",
        "    sen1_feats = { f\"{k}1\": v for k, v in anno1.items() if k in sen_feats }\n",
        "    sen2_feats = { f\"{k}2\": v for k, v in anno2.items() if k in sen_feats }\n",
        "    # not very smart idea: putting single sentence-based features as pair features\n",
        "    features = {**sen1_feats, **sen2_feats} # merge two dicts\n",
        "\n",
        "    # pair-related features\n",
        "    # if only one of the sentences has a negation\n",
        "    neg_set = set([anno1['neg_num'], anno2['neg_num']])\n",
        "    features['neg_diff'] = 1 if (0 in neg_set and len(neg_set) > 1) else 0\n",
        "\n",
        "    # If premise contains more tokens than hypothesis has\n",
        "    features['tok1>2'] = int(anno1['tok_num'] > anno2['tok_num'])\n",
        "\n",
        "    # If premise has more nouns than hypothesis has\n",
        "    features['noun1>2'] = int(anno1['noun_num'] > anno2['noun_num'])\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "fshUfLIdPsyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can modify the function, but it might not be necessary as it is pretty general\n",
        "def problems2df(data_dict, sen2af):\n",
        "    '''\n",
        "    Read a dictionary of NLI problems {pid->prob} and\n",
        "    a dictionary of sentence annotations {sent->anno_feats}\n",
        "    and represent each problem as a set of feature-values in DatFrame.\n",
        "    DataFrame offers an easy way of viewing and manipulating data.\n",
        "    Separate DataFrames are created for labels, features, and sentence pairs\n",
        "    https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
        "    '''\n",
        "    dict_of_feats = { pid: problem2features(prob['p'], sen2af[prob['p']], prob['h'], sen2af[prob['h']])\n",
        "                      for pid, prob in tqdm(data_dict.items()) }\n",
        "    # Use  {key : list or dict} to create DataFrame\n",
        "    gold_labels = { pid:[prob['g']] for pid, prob in data_dict.items() }\n",
        "    # Don't use label annotations as features as this will be cheating :)\n",
        "    # Create DataFrame for sentence pairs for visualization\n",
        "    pair_df = { pid:[f\"{prob['p']} ??? {prob['h']}\"] for pid, prob in tqdm(data_dict.items()) }\n",
        "    # make each problem charactersistics as a row\n",
        "    feat_df = pd.DataFrame(dict_of_feats).transpose()\n",
        "    lab_df = pd.DataFrame(gold_labels).transpose()\n",
        "    pair_df = pd.DataFrame(pair_df).transpose()\n",
        "    # match the order in label, feature and pair farmes\n",
        "    lab_df.reindex(feat_df.index)\n",
        "    pair_df.reindex(feat_df.index)\n",
        "    return feat_df, lab_df, pair_df"
      ],
      "metadata": {
        "id": "W5BBptMfUGD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_df, lab_df, pair_df = problems2df(SNLI['sub_train'], s2af)"
      ],
      "metadata": {
        "id": "kUz1uh9WqFSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's put all three dataframes together for visualization\n",
        "# Press the magic wand icon after the frame appears\n",
        "pd.concat([lab_df, feat_df, pair_df], axis=1)"
      ],
      "metadata": {
        "id": "s1eQ5ZHkrZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "8A91PFGE0t3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just an example with decision trees\n",
        "from sklearn.tree import DecisionTreeClassifier as DTC\n",
        "\n",
        "# preparing data and converting it to feature-values\n",
        "s2af = { s: sen2features(s, a) for s, a in tqdm(sub_S2A.items()) }\n",
        "feat_df, lab_df, pair_df = problems2df(SNLI['sub_train'], s2af)\n",
        "\n",
        "# initializing a DT classifier and training it\n",
        "DT = DTC(criterion=\"gini\", max_depth=10, random_state=0)\n",
        "default_DT = DT.fit(feat_df, lab_df)\n",
        "\n",
        "MODEL = {'cheater': default_DT}"
      ],
      "metadata": {
        "id": "TNIZX1_mDsP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "0rzwpL8I0yW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, sen2anno):\n",
        "    \"\"\"\n",
        "    model - a classifier to predict NLI classes\n",
        "    dataset and sem2anno are the similar to the output of snli_jsonl2dict\n",
        "    dataset - a dict of nli problems: keys are problem ids and values problem descriptions\n",
        "    sen2anno - a dict of sentence annotations from SNLI: keys are sentences and values its tree, pos tag and tokenozation.\n",
        "    The function converts problems in dataset into set of feature-values (sen2anno can be used reprocess each sentence once)\n",
        "    and predicts the inference classes of the problems.\n",
        "    It can use spacy model \"NLP\" on-fly to get features based on its analyses.\n",
        "    Returns a list of predictions and a list of gold values\n",
        "    \"\"\"\n",
        "\n",
        "    # a sample code wich is adapted to the previous code about decision trees\n",
        "    s2af = { s: sen2features(s, a) for s, a in tqdm(sen2anno.items()) }\n",
        "    feat_df, lab_df, pair_df = problems2df(dataset, s2af)\n",
        "\n",
        "    pred_list = model.predict(feat_df)\n",
        "    return pred_list.tolist(), lab_df.values.squeeze().tolist()"
      ],
      "metadata": {
        "id": "oQTcbobJqjp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "from nltk.metrics.scores import accuracy as Accuracy\n",
        "from nltk.metrics import ConfusionMatrix\n",
        "\n",
        "S2A_dev = sen2anno_from_nli_problems(SNLI['dev'], S2A)\n",
        "# The code should also work for 'test' part\n",
        "\n",
        "for name in MODEL:\n",
        "    pred, gold = evaluate(MODEL[name], SNLI['dev'], S2A_dev)\n",
        "    print(f\"{name:=^80}\")\n",
        "    print(ConfusionMatrix(gold, pred))\n",
        "    print(f\"Accuracy = {Accuracy(gold, pred)}\")\n",
        "    print(f\"{'':=^80}\")"
      ],
      "metadata": {
        "id": "izU3bUm6tHSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text2FOL with LogicLLaMa\n",
        "\n",
        "The code follows the content of the [demo notebook](https://github.com/gblackout/LogicLLaMA/blob/main/demo.ipynb).\n",
        "\n",
        "<font color=\"red\">Run the cells on GPU, e.g., the cells were tested on T4.</font>"
      ],
      "metadata": {
        "id": "zwfCp-BNyQpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the repo has this in requirements (might be relevant only for replicating results)\n",
        "! pip install transformers@git+https://github.com/huggingface/transformers.git@3ec7a47664ebe40c40f4b722f6bb1cd30c3821ec"
      ],
      "metadata": {
        "id": "6gfI_KqZ8zXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you might need to specify exact versions of the modules from requirements.txt\n",
        "! pip install peft Levenshtein SentencePiece bitsandbytes"
      ],
      "metadata": {
        "id": "WM2agHNr0zi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -fr /content/drive/MyDrive/Llama/LogicLLaMA\n",
        "! git clone https://github.com/gblackout/LogicLLaMA.git"
      ],
      "metadata": {
        "id": "uIB8FIzCyl5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"LogicLLaMA\")"
      ],
      "metadata": {
        "id": "dgsR04esysey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from functools import partial\n",
        "import transformers\n",
        "print(f\"transformers version={transformers.__version__}\")\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel, prepare_model_for_int8_training\n",
        "from utils import TranslationDataPreparer, ContinuousCorrectionDataPreparer, make_parent_dirs\n",
        "from fol_parser import parse_text_FOL_to_tree\n",
        "from generate import llama_generate"
      ],
      "metadata": {
        "id": "HQBCzvQOyyOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download data: MALLS and FOLIO\n",
        "! sh data_download.sh"
      ],
      "metadata": {
        "id": "R_6-8j501ap6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_ACCESS_TOKEN=\"hf_{SOME_MESS_OF_ALPHANUMERICS}\""
      ],
      "metadata": {
        "id": "C_Al6AUn2eGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA2_MODEL = 'meta-llama/Llama-2-7b-hf'"
      ],
      "metadata": {
        "id": "bqni2ImkzDg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_path='data/prompt_templates'\n",
        "load_in_8bit = True\n",
        "max_output_len = 128"
      ],
      "metadata": {
        "id": "YdUThVtF3b6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(LLAMA2_MODEL, use_auth_token=HF_ACCESS_TOKEN)\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": '<unk>',\n",
        "    \"pad_token\": '<unk>',\n",
        "})\n",
        "tokenizer.padding_side = \"left\"  # Allow batched inference"
      ],
      "metadata": {
        "id": "rBP9O4n-994j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=1\n",
        ")\n",
        "\n",
        "llama_model = LlamaForCausalLM.from_pretrained(\n",
        "    LLAMA2_MODEL,\n",
        "    load_in_8bit=load_in_8bit,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        "    use_auth_token=HF_ACCESS_TOKEN\n",
        ")\n",
        "llama_model = prepare_model_for_int8_training(llama_model)"
      ],
      "metadata": {
        "id": "zJkBN6Rw7kT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && git clone https://huggingface.co/yuan-yang/LogicLLaMA-7b-direct-translate-delta-v0.1"
      ],
      "metadata": {
        "id": "FAFsnJTQHmRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path='../LogicLLaMA-7b-direct-translate-delta-v0.1'"
      ],
      "metadata": {
        "id": "XFTNp-0Yle2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(\n",
        "    llama_model,\n",
        "    peft_path,\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "jDaM3v3YnY1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_preparer = TranslationDataPreparer(\n",
        "    prompt_template_path,\n",
        "    tokenizer,\n",
        "    False,\n",
        "    256 # just a filler number\n",
        ")\n",
        "\n",
        "prepare_input = partial(\n",
        "    data_preparer.prepare_input,\n",
        "    **{\"nl_key\": \"NL\"},\n",
        "    add_eos_token=False,\n",
        "    eval_mode=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "simple_generate = partial(\n",
        "    llama_generate,\n",
        "    llama_model=model,\n",
        "    data_preparer=data_preparer,\n",
        "    max_new_tokens=max_output_len,\n",
        "    generation_config=generation_config,\n",
        "    prepare_input=prepare_input,\n",
        "    return_tensors=False\n",
        ")"
      ],
      "metadata": {
        "id": "SM7_FnPaoSHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_point = {'NL': 'The one who created this repo is either a human or an alien'}"
      ],
      "metadata": {
        "id": "vGA1t-UVogis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_resp_str, resp_parts = simple_generate(input_str=data_point)"
      ],
      "metadata": {
        "id": "cA1-GjJ8osZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_parts"
      ],
      "metadata": {
        "id": "rRJ5tH2UpEG2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}